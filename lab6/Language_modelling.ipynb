{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wigDr73TZyW"
      },
      "source": [
        "# Language modelling\n",
        "The exercise shows how a language model may be used to solve word-prediction tasks and to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24JWZdG-TgEl"
      },
      "source": [
        "## Download three Polish models from the Huggingface repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHz-0sSjTQzb",
        "outputId": "6f69d190-d46b-4bbc-90c2-53800bfaafee"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 504 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdeYWjv-To90"
      },
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3YJicjqTqK0",
        "outputId": "8af7a74f-cfe6-4b31-bb53-9564cd080d14"
      },
      "source": [
        "herbert_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
        "herbert_model = AutoModelForMaskedLM.from_pretrained(\"allegro/herbert-base-cased\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForMaskedLM: ['cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouad-fjbT6eB",
        "outputId": "77dc8e53-4ea9-4e5d-f863-5423efa69a02"
      },
      "source": [
        "large_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-large-cased\")\n",
        "large_model = AutoModelForMaskedLM.from_pretrained(\"allegro/herbert-large-cased\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForMaskedLM: ['cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hilanNIkT7OE",
        "outputId": "e8118989-1cb1-43e1-c016-4a70de11252d"
      },
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
        "bert_model = AutoModelForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-JtyYyYHAc"
      },
      "source": [
        "def print_top5_unmask(tokenizer, model, sequence):\n",
        "  sequence = sequence.replace('[MASK]', tokenizer.mask_token)\n",
        "  inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "  mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "  token_logits = model(**inputs).logits\n",
        "  mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "  top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "  for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LWWEXJrYp_T"
      },
      "source": [
        "## Produce the predictions for the following sentences (use each model and check 5 predictions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-EDPIZ-Yoix"
      },
      "source": [
        "sequences = [\n",
        "             \"Warszawa to największe [MASK].\",\n",
        "             \"Te zabawki należą do [MASK].\",\n",
        "             \"Policjant przygląda się [MASK].\",\n",
        "             \"Na środku skrzyżowania widać [MASK].\",\n",
        "             \"Właściciel samochodu widział złodzieja z [MASK].\",\n",
        "             \"Prezydent z premierem rozmawiali wczoraj o [MASK].\",\n",
        "             \"Witaj drogi [MASK].\"\n",
        "]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIuActldZSJV"
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, name, tokenizer, model):\n",
        "    self.name = name\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model = model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpO0w-RDZgiH"
      },
      "source": [
        "models = [\n",
        "          Model('herbert-base-cased', herbert_tokenizer, herbert_model), \n",
        "          Model('herbert-large-cased', large_tokenizer, large_model),\n",
        "          Model('bert-base-polish-cased-v1', bert_tokenizer, bert_model)\n",
        "          ]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEmecl8XZ6sM",
        "outputId": "62ac42b5-69c2-45f5-a0be-7145f014abe5"
      },
      "source": [
        "for model in models:\n",
        "  print('-'*100)\n",
        "  print(model.name)\n",
        "  print()\n",
        "  for sequence in sequences:\n",
        "    print_top5_unmask(model.tokenizer, model.model, sequence)\n",
        "    print()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-base-cased\n",
            "\n",
            "Warszawa to największe miasto.\n",
            "Warszawa to największe lotnisko.\n",
            "Warszawa to największe centrum.\n",
            "Warszawa to największe miasta.\n",
            "Warszawa to największe atrakcje.\n",
            "\n",
            "Te zabawki należą do rodziny.\n",
            "Te zabawki należą do nas.\n",
            "Te zabawki należą do nich.\n",
            "Te zabawki należą do najlepszych.\n",
            "Te zabawki należą do ..\n",
            "\n",
            "Policjant przygląda się mężczyźnie.\n",
            "Policjant przygląda się kobiecie.\n",
            "Policjant przygląda się mu.\n",
            "Policjant przygląda się dziewczynie.\n",
            "Policjant przygląda się sprawie.\n",
            "\n",
            "Na środku skrzyżowania widać rondo.\n",
            "Na środku skrzyżowania widać samochody.\n",
            "Na środku skrzyżowania widać radiowóz.\n",
            "Na środku skrzyżowania widać samochód.\n",
            "Na środku skrzyżowania widać wiadukt.\n",
            "\n",
            "Właściciel samochodu widział złodzieja z samochodu.\n",
            "Właściciel samochodu widział złodzieja z włamaniem.\n",
            "Właściciel samochodu widział złodzieja z auta.\n",
            "Właściciel samochodu widział złodzieja z kierowcą.\n",
            "Właściciel samochodu widział złodzieja z parkingu.\n",
            "\n",
            "Prezydent z premierem rozmawiali wczoraj o przyszłości.\n",
            "Prezydent z premierem rozmawiali wczoraj o Polsce.\n",
            "Prezydent z premierem rozmawiali wczoraj o bezpieczeństwie.\n",
            "Prezydent z premierem rozmawiali wczoraj o polityce.\n",
            "Prezydent z premierem rozmawiali wczoraj o Warszawie.\n",
            "\n",
            "Witaj drogi Łukasz.\n",
            "Witaj drogi Boże.\n",
            "Witaj drogi człowieku.\n",
            "Witaj drogi Karol.\n",
            "Witaj drogi Marcin.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-large-cased\n",
            "\n",
            "Warszawa to największe miasto.\n",
            "Warszawa to największe miasta.\n",
            "Warszawa to największe ..\n",
            "Warszawa to największe lotnisko.\n",
            "Warszawa to największe miast.\n",
            "\n",
            "Te zabawki należą do dzieci.\n",
            "Te zabawki należą do mnie.\n",
            "Te zabawki należą do nas.\n",
            "Te zabawki należą do najmłodszych.\n",
            "Te zabawki należą do Ciebie.\n",
            "\n",
            "Policjant przygląda się sprawie.\n",
            "Policjant przygląda się sytuacji.\n",
            "Policjant przygląda się zdarzeniu.\n",
            "Policjant przygląda się wszystkiemu.\n",
            "Policjant przygląda się kobiecie.\n",
            "\n",
            "Na środku skrzyżowania widać rondo.\n",
            "Na środku skrzyżowania widać krzyż.\n",
            "Na środku skrzyżowania widać radiowóz.\n",
            "Na środku skrzyżowania widać samochód.\n",
            "Na środku skrzyżowania widać znak.\n",
            "\n",
            "Właściciel samochodu widział złodzieja z bronią.\n",
            "Właściciel samochodu widział złodzieja z bliska.\n",
            "Właściciel samochodu widział złodzieja z kamerą.\n",
            "Właściciel samochodu widział złodzieja z nożem.\n",
            "Właściciel samochodu widział złodzieja z kamery.\n",
            "\n",
            "Prezydent z premierem rozmawiali wczoraj o tym.\n",
            "Prezydent z premierem rozmawiali wczoraj o sprawie.\n",
            "Prezydent z premierem rozmawiali wczoraj o sytuacji.\n",
            "Prezydent z premierem rozmawiali wczoraj o prywatyzacji.\n",
            "Prezydent z premierem rozmawiali wczoraj o ..\n",
            "\n",
            "Witaj drogi człowieku.\n",
            "Witaj drogi Panie.\n",
            "Witaj drogi Jezu.\n",
            "Witaj drogi panie.\n",
            "Witaj drogi misiu.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "bert-base-polish-cased-v1\n",
            "\n",
            "Warszawa to największe miasto.\n",
            "Warszawa to największe województwo.\n",
            "Warszawa to największe lotnisko.\n",
            "Warszawa to największe miasteczko.\n",
            "Warszawa to największe państwo.\n",
            "\n",
            "Te zabawki należą do ciebie.\n",
            "Te zabawki należą do mnie.\n",
            "Te zabawki należą do nas.\n",
            "Te zabawki należą do pana.\n",
            "Te zabawki należą do niego.\n",
            "\n",
            "Policjant przygląda się temu.\n",
            "Policjant przygląda się sprawie.\n",
            "Policjant przygląda się im.\n",
            "Policjant przygląda się wszystkiemu.\n",
            "Policjant przygląda się panu.\n",
            "\n",
            "Na środku skrzyżowania widać rzekę.\n",
            "Na środku skrzyżowania widać ulicę.\n",
            "Na środku skrzyżowania widać drzewa.\n",
            "Na środku skrzyżowania widać drogę.\n",
            "Na środku skrzyżowania widać las.\n",
            "\n",
            "Właściciel samochodu widział złodzieja z bronią.\n",
            "Właściciel samochodu widział złodzieja z tyłu.\n",
            "Właściciel samochodu widział złodzieja z ulicy.\n",
            "Właściciel samochodu widział złodzieja z bliska.\n",
            "Właściciel samochodu widział złodzieja z zewnątrz.\n",
            "\n",
            "Prezydent z premierem rozmawiali wczoraj o tym.\n",
            "Prezydent z premierem rozmawiali wczoraj o Polsce.\n",
            "Prezydent z premierem rozmawiali wczoraj o budżecie.\n",
            "Prezydent z premierem rozmawiali wczoraj o ASF.\n",
            "Prezydent z premierem rozmawiali wczoraj o ustawie.\n",
            "\n",
            "Witaj drogi chłopcze.\n",
            "Witaj drogi przyjacielu.\n",
            "Witaj drogi bracie.\n",
            "Witaj drogi kolego.\n",
            "Witaj drogi synu.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD7VnT-Qa4Ks"
      },
      "source": [
        "## Check the model predictions for the following sentences (using each model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH-IbF24a6jv"
      },
      "source": [
        "sequences = [\n",
        "             \"Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\",\n",
        "             \"Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie [MASK].\"\n",
        "]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7ZZD9BQbB30",
        "outputId": "35f5c84c-c6a1-47ea-d517-41edbd6ddffa"
      },
      "source": [
        "for model in models:\n",
        "  print('-'*100)\n",
        "  print(model.name)\n",
        "  print()\n",
        "  for sequence in sequences:\n",
        "    print_top5_unmask(model.tokenizer, model.model, sequence)\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-base-cased\n",
            "\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie poddał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zdziwił.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie dowiedział.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zastanawiał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie przyznał.\n",
            "\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie dowiedziała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie przyznała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie bała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zmieniła.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zgodziła.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-large-cased\n",
            "\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie bał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie poddał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie śmiał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zastanawiał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zabił.\n",
            "\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie bała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zgodziła.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie dowiedziała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zmieniła.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie śmiała.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "bert-base-polish-cased-v1\n",
            "\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zgodził.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie bał.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie dowiedział.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie pojawił.\n",
            "Gdybym wiedział wtedy dokładnie to, co wiem teraz, to bym się nie zabił.\n",
            "\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zgodziła.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie bała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie dowiedziała.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie pojawiła.\n",
            "Gdybym wiedziała wtedy dokładnie to, co wiem teraz, to bym się nie zabiła.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Byck2IObK_N"
      },
      "source": [
        "## Check the model predictions for the following sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwMjLw15bNMz"
      },
      "source": [
        "sequences = [\n",
        "             \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\",\n",
        "             \"W wakacje odwiedziłem [MASK], który jest stolicą Islandii.\",\n",
        "             \"Informatyka na [MASK] należy do najlepszych kierunków w Polsce.\"\n",
        "]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW_gy-kzbUD-",
        "outputId": "6ab8ce72-7037-4f9f-832f-71988df425e5"
      },
      "source": [
        "for model in models:\n",
        "  print('-'*100)\n",
        "  print(model.name)\n",
        "  print()\n",
        "  for sequence in sequences:\n",
        "    print_top5_unmask(model.tokenizer, model.model, sequence)\n",
        "    print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-base-cased\n",
            "\n",
            "Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Słońce wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Ziemia wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Następnie wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Ciało wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "\n",
            "W wakacje odwiedziłem Kraków, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Oslo, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Londyn, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Gdańsk, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Toruń, który jest stolicą Islandii.\n",
            "\n",
            "Informatyka na pewno należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na AGH należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na UW należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na studiach należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na UMK należy do najlepszych kierunków w Polsce.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "herbert-large-cased\n",
            "\n",
            "Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Krew wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Nie wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Ogień wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "\n",
            "W wakacje odwiedziłem Oslo, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Londyn, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Liverpool, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Glasgow, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Birmingham, który jest stolicą Islandii.\n",
            "\n",
            "Informatyka na AGH należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na UW należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na UJ należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na UAM należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na uczelni należy do najlepszych kierunków w Polsce.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "bert-base-polish-cased-v1\n",
            "\n",
            "Woda wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Mięso wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Słońce wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Nie wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "Ziemia wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\n",
            "\n",
            "W wakacje odwiedziłem kraj, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Cypr, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Meksyk, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Gibraltar, który jest stolicą Islandii.\n",
            "W wakacje odwiedziłem Wellington, który jest stolicą Islandii.\n",
            "\n",
            "Informatyka na wsi należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na świecie należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na żywo należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na pewno należy do najlepszych kierunków w Polsce.\n",
            "Informatyka na odległość należy do najlepszych kierunków w Polsce.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgdsL2wtcb3X"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "    \n",
        "\n",
        "1.   Which of the models produced the best results? *Herbert-large-cased*\n",
        "2.   Was any of the models able to capture Polish grammar? *All*\n",
        "3.   Was any of the models able to capture long-distant relationships between the words? *Herbert-large-cased*\n",
        "4.   Was any of the models able to capture world knowledge? *All (water)*\n",
        "5.   What are the most striking errors made by the models? \"Warszawa to największe miasta.\" or replacing [MASK] with \".\" \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKyfnX2MeHEC"
      },
      "source": [
        "Wykonał **Mikołaj Sikora**\n",
        "\n",
        "Grupa 12:50 Piątek"
      ]
    }
  ]
}